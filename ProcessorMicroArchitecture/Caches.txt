

 CACHES
 --------------

 NOTE: Spatial refers to space
       Temporal refers to time
 
 By buffering such frequently accesssed data in a small and fast structure, the processor can give
 the illusion to the application that accesses to main memory are in the order of a few cycles.

 Moreover, although each core in a multicore processor has its own (private) first-level caches,
 higher levels of the memory hierarchy are usually shared among multiple cores.

 Normally, program data and instruction addresses are virtual addresses. Load and store instructions
 as well as the fetch engine must perform address translation (more on this later) to translate
 the virtual addresses to physical addresses.
 The caches can be indexed with either virtual or physical addresses.
 In the former case, the cache access can be initiated earlier, since the address translation can 
 be performed in parallel with the cache access. To deal with potential aliasing problems, the 
 tags are normally generated from the physical address.


 ADDRESS TRANSLATION
 -------------------------
 
 The physical address space is defined as the range of addresses that the processor can generate on 
 its bus. The virtual address space is the range of addresse than an application program can use.

 Virtualizing the program addresses servies two main purposes:
 1. It allows each program to run unmodified on machiens with different amounts of physical memory
    installed or on multitasking systems where physical memory is shared among many applications.
 2. Second, by isolating the virtual address spaces of different programs, we can protect applications
    from each other on multitasking systems.

 The virtualization of the linear address space is handled through the processor's paging mechanism.
 When using paging, the address space is divided into pages (typically 4-8 KB in size). A page can
 reside either in the main memory or in the disk (a swapped-out page). The operating system maintains
 a mapping of virtual pages to physical pages through a structure called the page table.
 The page table usually is stored in the main memory.

 It is possible for two virtual pages to map to the same physical page. This is typical, for 
 example, in shared-memory multithreaded applications. When this occurs, the two virtual pages
 are "aliases" of the same physical entity, so this is called virtual aliasing.
 When implementing the data cache, load/store buffers and all related management mechanisms, it
 is very important to be able to handle virtual aliasing correctly.

 When a program issues a load or store instruction, or an instruction fetch, the page map must be
 consulted to translate the linear address to a physical address before the memory access can be
 performed. A linear address is divided into two parts: the page offset and the page number. 
 The page offset corresponds to the N least significant bits of the address -
 where N = log2(PAGE_SIZE) - and identifices a byte address inside a page. The page number - the
 remaining address bits - identifices a page inside the address space.

 In a naive implementation, a load instruction (or instruction fetch) would have to perform several
 memory accesses in order to translate the linear address to a physical one (the page table is in 
 main memory). Since this is a critical operation, all modern processors implement a page table
 cache, called the translation lookaside buffer (TLB). The TLB is a small hardware cache structure
 that only caches page table entries.

 In some processors, such as the Alpha series of processors, the TLB is entirely software controlled.
 This means that the operating system has total freedom in how to organize the page map in main
 memory and also total control of which mappings it wants to be cached to the TLB (there are special
 instructions to add/remove entries from the TLB).

 In the x86 architecture. The TLB is hardware controlled and is mostly transparent to the operating
 system. In this architecture, the page map has a specific format that the hardware is able to 
 understand. It is the operating system's responsibility to create the page map in a place in the 
 memory where the hardware can find it and in the right format so that the hardware can parse it.

 The TLB usually contains in the order of a few tens to a few hundred entries. Associativity
 may vary, but since it is cretical for performance, its access time is usually a single cycle.
 The TLB is always indexed by the page number of the virtual address, and it returns the
 corresponding physical page number and some information for this page. The informatmion includes the
 access rights for the page (if it can be read or written or if it is executable), and it is 
 mapped to a main memory page or if it backed up in permanent storage.


 CACHE STRUCTURE ORGANIZATION
 ------------------------------
 
 Address decoder: an address decoder is a binary decoder that has two or more inputs for address bits
 	and one or more outputs for device selection signals. When the address for a particular
	device appears on the address inputs, the decoder asserts the selection ouput for that
	device. A dedicated, single-output address decoder may be incorporated into each device
	on an address bus, or a single address decoder may serve multiple devices.


 A single address decoder with n address input bits can serve up to 2^n devices. Several members
 of the 7400 series of integrated circuits can be used as address decoders.
 For example, when used as an address decoder, the 74154 provides four address inputs and sixteen
 device selector outputs. An address decoder is a particular use of a binary decoder circuit
 known as a "demultiplexer" or "demux" (the 74154 is commonly called a "4-to-16 demultiplexer"),
 which has many other uses besides address decoding.

 Address decoder selects the storage cell in a memory:

 An address decoder is a commonly used component in microelectronics that is used to select
 memory cells in randomly addressable memory devices.

 Such a memory cell consists of a fixed number of memory elements or bits. The address decoder is 
 connected to an address bus and reads the address created there. Using a special switching logic,
 it uses this address to calculate which memory cell is t o be accessed. It then selects that
 cell by selecting it via a special control line. This line is also known as the select line. 
 In dynamic memories (DRAM), there are row and column select lines on the memory matrix, 
 which are controlled by address decoders integrated in the chip.

 Depending on the type of decoder, the logic used to select the memory cell can under certain
 circumstances be programmable.

 Address decoder selects the appropriate memory module:

 An address decoder is also used to select the appropriate one of multiple memory modules or 
 memory chips when a particular address is provided by the processor system's address bus.

 For this purpose, the memory modules or memory chips have selection inputs, usually referred to
 as chip select pin (CS) or chip enable pin (CE) pin. These inputs often have a negative logic
 function.

 The address decoder uses the different combinatorial logic to place the memory modules or chips
 in the address space of a processor. The memory modules often have a smaller capacity than the
 address space. In most cases, several modules can be used, even if they are completely identical
 in structure. It must be ensured that they differ in the address range.

 Binary decoder:

 Binary decoder is a combinational logic circuit that converts binary information from the n coded
 inputs to a maximum of 2^n unique outputs. They are used in a wide variety of applications,
 including instruction decoding, data multiplexing and data demultiplexing, seven segment displays,
 and as address decoders for memory and port-mapped I/O.

 There are several types of binary decoders, but in all cases a decoder is an electronic circuit
 with multiple input and multiple output signals, which converts every unioque combination of 
 input states to a specific combination of output states. In addition to integer data inputs, 
 some decoders also have one or more "enable" inputs. When the enable input is negated (disabled),
 all decoder outputs are forced tot  their inactive states.

 Depending on its function, a binary decoder will convert binary information from n input signals
 to as many as 2^n unique output signals. Some decoders have less than 2^n output lines; in such
 cases, at least one output pattern may be repeated for different input values.

 A binary deocder is usually implemented as either a stand-alone integrated circuit (IC) or as part
 of a more complex IC. In the latter case the decoder may be synthesized by means of a hardware
 description language such as VHDL or Verilog. Widely used decoders are often available in the form
 of standardized ICs.

 Cache organization:

 A cache consists of two main blocks: the tag array and the data array. The data array stores the 
 application data or instructions, while the tag array is used by the processor in order to match
 application addresses into data array entries.

 The data array logically is organized as a group of sets. Each set is a collection of blocks.
 The number of blocks in a set is called the degree of associativity of the cache. We also say that
 a cache of associativity N is an N-way associative cache. The i-th cache way is defined to be the
 collection of the i-th blocks of all sets in a cache. A case with an associativity degree of 1 is
 called a directed mapped cache.

 The memory address:
 1. The K least significant bits of the address are used to identify which bytes insie a cache block
    we want to access. This part of the address is called the block offset.
    Assuming the block size is Q bytes, then K = log2(Q)
 2. The next part of the address is called the index. As its name denotes, the inex is use to 
    identify the position of a set into the data array. For a data cache of S sets, we need
    M = log2(S) bits of index.
 3. For each block in the data array, the tag array holds some metadata: the rest of the address bits
    and the state of the block (valid, etc.)

 Different addresses can map to the same set in the data cache (they have the same index), so 
 we need a mechanism to reverse-map indexes to addresses. The tag array serves this purpose.
 The tag array has the same logical organization as the data array (same number of sets and 
 associativity).

 A memory requrest accesses both the data and the tag arrays using the index part of the address,
 but in order to know if the block accessed corresponds to the given address, it must match the 
 rest of the address bits with the tag bits. If the tag bits of the i-th block in the accessed set
 match, then the correct data is in the i-th block of the corresponding data array set (this is
 called a cache hit). If no tags in the set match the incoming address, then the requested data
 does not reside in the cache (this is a cache miss), and a request to the higher levels of the
 memory hierarchy must be issued and wait for the data to be installed in the cache before the
 access can proceed.

 Parallel Tag and Data Array Access
 ------------------------------------

 The access to the tag and data array can occur in parallel or serially.
 
 In the first case, a whole set is read from the data array while the tag array is accessed.
 The address is compared with the tag entries to find in which block of the set reside the data
 that we are looking for. This information is fed to a multiplexor at the output of the data
 array (the way multiplexor) that chooses one of the blocks of the set. Finally, the offset part of 
 the address is used to extract the appropriate bytes from the chosen block (this process is called
 data alignment).

 The cache access is typically one of the critical paths in a processor; thus, for high-frequency
 machines, it is pipelined.

 So in the second case, There are two critical paths. The one is the path that goes through the tag
 array, the tag comparison and the way multiplexor control. The other is the one that goes through
 the data array and the way multiplexor data path.

